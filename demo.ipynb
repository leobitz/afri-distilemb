{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "840d80c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modeling_distillemb import BertModel, BertForSequenceClassification, BertForEmbeddingLM\n",
    "from distill_emb import DistillEmbSmall, DistillEmb\n",
    "from config import DistillModelConfig, DistillEmbConfig\n",
    "import torch\n",
    "from transformers import AutoTokenizer, RwkvConfig, RwkvModel, AutoModel\n",
    "from tokenizer import CharTokenizer\n",
    "from knn_classifier import KNNTextClassifier\n",
    "from data_loader import load_sentiment, load_ner_dataset, load_pos_dataset\n",
    "from data_loader import load_news_dataset\n",
    "import pandas as pd\n",
    "from retrieval import build_json_pairs, top1_accuracy\n",
    "import os\n",
    "from transformers import GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dccc0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 105862 rows from sentiment.parquet columns Index(['text', 'label', 'lang', 'split'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df, classes = load_sentiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4afb114a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>lang</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tesfaye ·àà·ä´·àµ ·å≠·â•·àç ·àà·â•·à∞·àΩ ·ã®·çï·àÆ·çå·à∞·à≠·äï ·çé·â∂ ·àà·å•·çà·ä≠ ·ä•·àç·àù ·ã´·àç·ä≠ ·â£...</td>\n",
       "      <td>negative</td>\n",
       "      <td>am</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>·ã≠·àÑ·ãç ·äê·ãç ·ä†·ã≠·ã∞·àç ·ã®·ä•·ãç·âÄ·âµ·àΩ ·å•·åç....·â†·à∞·àö ·à∞·àö ·ä®·àù·âµ·äì·åà·à™ ·àà·àù·äï ·â≥·à™·ä≠...</td>\n",
       "      <td>negative</td>\n",
       "      <td>am</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>·ãò·åà·â† ·ã≠·â£·àã·àç? ·àå·àã ·ã®·àö·â£·àç ·äê·åà·à≠ ·ä´·àà ·ä†·äï·â∞·ãâ ·äï·åà·à®·äï!</td>\n",
       "      <td>negative</td>\n",
       "      <td>am</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>?? ·ãµ·àÆ ·â†·ãò·àò·äê ·äÆ·ã≥·ä≠ ·çé·â∂ ·â§·âµ ·çç·àã·àπ ·çè ·à≤·àç ·ä†·ã≠·äì·âΩ·äï ·â∞·å®·çç·äñ ·ä•·äï·ã≥·ã≠·ãà...</td>\n",
       "      <td>negative</td>\n",
       "      <td>am</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>·ã†·àç·å•?? ???? ·åà·åà·àõ</td>\n",
       "      <td>negative</td>\n",
       "      <td>am</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105857</th>\n",
       "      <td>@user Taakkee Jabaadhu!!! olola gadi galoo hin...</td>\n",
       "      <td>positive</td>\n",
       "      <td>or</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105858</th>\n",
       "      <td>@user Waraana Bilisummaa Oromiyaa. Unity of Or...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>or</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105859</th>\n",
       "      <td>#Jawwaar dhugumatti hogganaa walitti-hidhaa ga...</td>\n",
       "      <td>negative</td>\n",
       "      <td>or</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105860</th>\n",
       "      <td>Yooyyaa Yooyyaa akkam jirtan sabni Oromo hundi...</td>\n",
       "      <td>negative</td>\n",
       "      <td>or</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105861</th>\n",
       "      <td>@user Kabajamoo kantibaa keenya ati hojii umma...</td>\n",
       "      <td>negative</td>\n",
       "      <td>or</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>105861 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text     label lang  \\\n",
       "1       Tesfaye ·àà·ä´·àµ ·å≠·â•·àç ·àà·â•·à∞·àΩ ·ã®·çï·àÆ·çå·à∞·à≠·äï ·çé·â∂ ·àà·å•·çà·ä≠ ·ä•·àç·àù ·ã´·àç·ä≠ ·â£...  negative   am   \n",
       "2       ·ã≠·àÑ·ãç ·äê·ãç ·ä†·ã≠·ã∞·àç ·ã®·ä•·ãç·âÄ·âµ·àΩ ·å•·åç....·â†·à∞·àö ·à∞·àö ·ä®·àù·âµ·äì·åà·à™ ·àà·àù·äï ·â≥·à™·ä≠...  negative   am   \n",
       "3                     ·ãò·åà·â† ·ã≠·â£·àã·àç? ·àå·àã ·ã®·àö·â£·àç ·äê·åà·à≠ ·ä´·àà ·ä†·äï·â∞·ãâ ·äï·åà·à®·äï!  negative   am   \n",
       "4       ?? ·ãµ·àÆ ·â†·ãò·àò·äê ·äÆ·ã≥·ä≠ ·çé·â∂ ·â§·âµ ·çç·àã·àπ ·çè ·à≤·àç ·ä†·ã≠·äì·âΩ·äï ·â∞·å®·çç·äñ ·ä•·äï·ã≥·ã≠·ãà...  negative   am   \n",
       "5                                          ·ã†·àç·å•?? ???? ·åà·åà·àõ  negative   am   \n",
       "...                                                   ...       ...  ...   \n",
       "105857  @user Taakkee Jabaadhu!!! olola gadi galoo hin...  positive   or   \n",
       "105858  @user Waraana Bilisummaa Oromiyaa. Unity of Or...   neutral   or   \n",
       "105859  #Jawwaar dhugumatti hogganaa walitti-hidhaa ga...  negative   or   \n",
       "105860  Yooyyaa Yooyyaa akkam jirtan sabni Oromo hundi...  negative   or   \n",
       "105861  @user Kabajamoo kantibaa keenya ati hojii umma...  negative   or   \n",
       "\n",
       "        split  \n",
       "1       train  \n",
       "2       train  \n",
       "3       train  \n",
       "4       train  \n",
       "5       train  \n",
       "...       ...  \n",
       "105857   test  \n",
       "105858   test  \n",
       "105859   test  \n",
       "105860   test  \n",
       "105861   test  \n",
       "\n",
       "[105861 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b9dc4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 30494 rows from masakhapos.parquet columns Index(['id', 'tokens', 'labels', 'split'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df, classes = load_pos_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53e6dda1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tokens</th>\n",
       "      <th>labels</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[Muso, ≈ãana, ,, Afiriki, tilebinyanfan, n', a,...</td>\n",
       "      <td>[0, 6, 1, 0, 0, 9, 11, 0, 14, 9, 0, 14, 17, 11...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[Ni, m…îg…î, ka, d…îg…în, kojugu, ,, i, b…õ, m…îg…îw,...</td>\n",
       "      <td>[9, 0, 7, 6, 0, 1, 11, 17, 0, 0, 16, 0, 7, 11,...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[A, k…îr…îtalen, a, ka, ta…≤…õ, f…õ, kow, …≤…õm…îg…îyab...</td>\n",
       "      <td>[11, 16, 11, 7, 0, 7, 0, 0, 7, 9, 0, 17, 11, 7...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[Ale, y', a, (, basik…õti, ), to, a, ka, se, ka...</td>\n",
       "      <td>[11, 7, 11, 1, 0, 1, 16, 11, 7, 16, 7, 16, 11,...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[Sannay…õl…õn, galabuk…õn…õya, a, sera, ka, min, f...</td>\n",
       "      <td>[0, 0, 11, 16, 7, 11, 16, 10, 0, 7, 9, 0, 1, 0...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30489</th>\n",
       "      <td>145</td>\n",
       "      <td>[Uthe, leli, cala, lisaphenywa, .]</td>\n",
       "      <td>[16, 8, 0, 16, 1]</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30490</th>\n",
       "      <td>146</td>\n",
       "      <td>[Uthe, kusolakala, ukuthi, kube, nezigilamkhub...</td>\n",
       "      <td>[16, 16, 5, 16, 0, 16, 0, 12, 10, 1]</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30491</th>\n",
       "      <td>147</td>\n",
       "      <td>[\"\"\"\", Kusolakala, ukuthi, umndeni, waseMange,...</td>\n",
       "      <td>[1, 16, 5, 0, 10, 1, 10, 16, 0, 16, 1]</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30492</th>\n",
       "      <td>148</td>\n",
       "      <td>[Babathumbile, beba, nezimoto, .]</td>\n",
       "      <td>[16, 16, 0, 1]</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30493</th>\n",
       "      <td>149</td>\n",
       "      <td>[Enye, yezimoto, igcine, itholakala, eManguzi,...</td>\n",
       "      <td>[11, 0, 16, 16, 10, 1, 1, 1, 16, 0, 10, 1]</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30494 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                             tokens  \\\n",
       "0        0  [Muso, ≈ãana, ,, Afiriki, tilebinyanfan, n', a,...   \n",
       "1        1  [Ni, m…îg…î, ka, d…îg…în, kojugu, ,, i, b…õ, m…îg…îw,...   \n",
       "2        2  [A, k…îr…îtalen, a, ka, ta…≤…õ, f…õ, kow, …≤…õm…îg…îyab...   \n",
       "3        3  [Ale, y', a, (, basik…õti, ), to, a, ka, se, ka...   \n",
       "4        4  [Sannay…õl…õn, galabuk…õn…õya, a, sera, ka, min, f...   \n",
       "...    ...                                                ...   \n",
       "30489  145                 [Uthe, leli, cala, lisaphenywa, .]   \n",
       "30490  146  [Uthe, kusolakala, ukuthi, kube, nezigilamkhub...   \n",
       "30491  147  [\"\"\"\", Kusolakala, ukuthi, umndeni, waseMange,...   \n",
       "30492  148                  [Babathumbile, beba, nezimoto, .]   \n",
       "30493  149  [Enye, yezimoto, igcine, itholakala, eManguzi,...   \n",
       "\n",
       "                                                  labels  split  \n",
       "0      [0, 6, 1, 0, 0, 9, 11, 0, 14, 9, 0, 14, 17, 11...  train  \n",
       "1      [9, 0, 7, 6, 0, 1, 11, 17, 0, 0, 16, 0, 7, 11,...  train  \n",
       "2      [11, 16, 11, 7, 0, 7, 0, 0, 7, 9, 0, 17, 11, 7...  train  \n",
       "3      [11, 7, 11, 1, 0, 1, 16, 11, 7, 16, 7, 16, 11,...  train  \n",
       "4      [0, 0, 11, 16, 7, 11, 16, 10, 0, 7, 9, 0, 1, 0...  train  \n",
       "...                                                  ...    ...  \n",
       "30489                                  [16, 8, 0, 16, 1]    dev  \n",
       "30490               [16, 16, 5, 16, 0, 16, 0, 12, 10, 1]    dev  \n",
       "30491             [1, 16, 5, 0, 10, 1, 10, 16, 0, 16, 1]    dev  \n",
       "30492                                     [16, 16, 0, 1]    dev  \n",
       "30493         [11, 0, 16, 16, 10, 1, 1, 1, 16, 0, 10, 1]    dev  \n",
       "\n",
       "[30494 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf287ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_input_chars=12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c07d547",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CharTokenizer(charset_file_path='tokenizer/charset.json',\n",
    "                          max_word_length=num_input_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0870d762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char_input shape: torch.Size([1, 10, 12])\n"
     ]
    }
   ],
   "source": [
    "config = DistillModelConfig(\n",
    "    vocab_size=30522,\n",
    "    hidden_size=768,\n",
    "    num_hidden_layers=9,\n",
    "    num_attention_heads=8,\n",
    "    intermediate_size=3072,\n",
    "    max_position_embeddings=512,\n",
    "    type_vocab_size=2,\n",
    "    pad_token_id=0,\n",
    "    position_embedding_type=\"absolute\",\n",
    "    use_cache=True,\n",
    "    classifier_dropout=None,\n",
    "    embedding_type=\"distill\",  # 'distilemb', 'fasttext'\n",
    "    encoder_type='bert', #'lstm'\n",
    "    num_input_chars=num_input_chars,  # number of characters in each token\n",
    "    char_vocab_size=tokenizer.char_vocab_size,\n",
    "    distil_config=DistillEmbConfig(\n",
    "        num_input_chars=tokenizer.max_word_length,  # number of characters in each token\n",
    "        char_vocab_size=tokenizer.char_vocab_size,\n",
    "        size=\"small\",\n",
    "        distill_dropout=0.1,\n",
    "        embedding_size=512,  # size of the embedding vector for each character\n",
    "    )\n",
    ")\n",
    "model = BertForSequenceClassification(config)\n",
    "model.bert.load_word_embeddings('/home/leo/project/distil-research/distilemb/logs/distill_emb_v0/distill_emb_v0-epoch=136-epoch_val_loss=0.27.ckpt')\n",
    "# input ids with (B, S, N)\n",
    "char_input = torch.randint(0, config.num_input_chars, (1, 10, config.num_input_chars))\n",
    "# input ids with (B, S, N)\n",
    "print(\"char_input shape:\", char_input.shape)\n",
    "inputs = {\n",
    "    \"input_ids\": char_input,\n",
    "    \"attention_mask\":torch.tensor([[1] * char_input.size(1)]),  # attention mask for each token\n",
    "    \"token_type_ids\": torch.tensor([[0] * char_input.size(1)]),  # token type ids for each token\n",
    "}\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2955b9a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0e8c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model checkpoint logs/distill_emb_v0/distill_emb_v0-epoch=95-epoch_val_loss=0.06.ckpt not found. Please check the path.\n"
     ]
    }
   ],
   "source": [
    "distill_emb = DistillEmb(config.distill_config)\n",
    "path = \"logs/distill_emb_v0/distill_emb_v0-epoch=95-epoch_val_loss=0.06.ckpt\"\n",
    "if os.path.exists(path):\n",
    "    state_dict = torch.load(path, map_location='cpu')['state_dict']\n",
    "    # remove 'model.' prefix from state_dict keys\n",
    "    state_dict = {k.replace('model.', ''): v for k, v in state_dict.items()}\n",
    "    distill_emb.load_state_dict(state_dict)\n",
    "else:\n",
    "    print(f\"Model checkpoint {path} not found. Please check the path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f387f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistillEmb(\n",
       "  (encoder): DistillEmbSmall(\n",
       "    (embedding): Embedding(1518, 64)\n",
       "    (conv1): Conv1d(12, 128, kernel_size=(5,), stride=(1,))\n",
       "    (conv2): Conv1d(128, 256, kernel_size=(5,), stride=(1,))\n",
       "    (conv3): Conv1d(256, 384, kernel_size=(5,), stride=(1,))\n",
       "    (conv4): Conv1d(384, 512, kernel_size=(4,), stride=(1,))\n",
       "    (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (activation): ReLU()\n",
       "    (tanh): Tanh()\n",
       "    (norm0): LayerNorm((12, 64), eps=1e-05, elementwise_affine=True)\n",
       "    (norm1): LayerNorm((128, 30), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((256, 13), eps=1e-05, elementwise_affine=True)\n",
       "    (norm3): LayerNorm((384, 4), eps=1e-05, elementwise_affine=True)\n",
       "    (norm4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (output_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distill_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a54f9283",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"Paris is the capital of France.\",\n",
    "    \"The Eiffel Tower is in Paris.\",\n",
    "    \"The Louvre Museum is in Paris.\",\n",
    "    \"The Seine River flows through Paris.\",\n",
    "    \"Paris is known for its art, fashion, and culture.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72f0f39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = tokenizer(texts, add_special_tokens=True, return_tensors='pt', padding='longest', truncation=True, max_length=32, return_attention_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c4b2e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 12, 12]), torch.Size([5, 12]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['input_ids'].shape, out['attention_mask'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f5db319",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForEmbeddingLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    }
   ],
   "source": [
    "emb_lm_model = BertForEmbeddingLM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af523e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = distill_emb(out['input_ids'])\n",
    "out['labels'] = labels\n",
    "\n",
    "output = emb_lm_model(**out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66d5ff39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 12, 512])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bb496821",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = tokenizer.encode(tokenizer.special_token2word['[PAD]'], add_cls=False, add_sep=False, return_attention_mask=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a8e493ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 12])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(out['input_ids']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0aeb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "distill_emb = distill_emb.to('cuda').eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51528542",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = KNNTextClassifier(tokenizer, model=distill_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e484af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, classes = load_sentiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5227365",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df.sample(1000, random_state=42)\n",
    "test_df = df.drop(train_df.index).sample(100, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7589f008",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.classifiy(train_df=train_df, test_df=test_df, k=5, batch_size=32, model=None, tokenizer=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07533b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bonadossou/afrolm_active_learning\"\n",
    "tok = AutoTokenizer.from_pretrained(model_name)\n",
    "xmodel = AutoModel.from_pretrained(model_name)\n",
    "class Wrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        embs = self.model(**kwargs).last_hidden_state\n",
    "        return embs\n",
    "\n",
    "wrapper_model = Wrapper(xmodel).to('cuda').eval()\n",
    "classifier = KNNTextClassifier(tokenizer=tok, model=wrapper_model)\n",
    "classifier.classifiy(train_df=train_df, test_df=test_df, k=5, batch_size=32, model=wrapper_model, tokenizer=tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2459c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, classes = load_news_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34322e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = data.sample(1000, random_state=42)\n",
    "test_df = data.drop(train_df.index).sample(100, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2893e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.classifiy(train_df=train_df, test_df=test_df, k=5, batch_size=32, model=None, tokenizer=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e4b474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select 10 per language\n",
    "train_df = data[data['split'] == 'train'].groupby('lang').apply(lambda x: x.sample(200, random_state=42)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81b19d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# langs = ['amh', 'hau', 'ibo', 'lug', 'pcm','yor']\n",
    "# train_df = train_df[train_df['lang'].isin(langs)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75066ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['headline'].sample(1).values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a1b56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = build_json_pairs(train_df, model_name=\"Davlan/afro-xlmr-large\",\n",
    "#                  n_samples=200, m_candidates=100, k_top=9, text_col=\"text\", headline_col=\"headline\")\n",
    "# # save to json file\n",
    "# import json\n",
    "# with open('news_result.json', 'w', encoding='utf-8') as f:\n",
    "#     json.dump(result, f, indent=4, ensure_ascii=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46753293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = build_json_pairs(train_df, model_name=\"Davlan/afro-xlmr-large\",\n",
    "#                  n_samples=200, m_candidates=100, k_top=9, text_col=\"headline\", headline_col=\"text\")\n",
    "# # save to json file\n",
    "# import json\n",
    "# with open('headline_result.json', 'w', encoding='utf-8') as f:\n",
    "#     json.dump(result, f, indent=4, ensure_ascii=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076d75e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('news_result.json')\n",
    "d = df.to_dict(orient='records')\n",
    "top1_accuracy(d, batch_size=32, model=xmodel, tokenizer=tok)\n",
    "# top1_accuracy(d, batch_size=32, model=distill_emb, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db7f662",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fasttext_model import FastTextModel\n",
    "fasttext_model = FastTextModel(file_path='embeddings/afriberta/afriberta.vec')\n",
    "# fasttext_model.embedding.weight.requires_grad = False  # freeze the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a441e21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
