{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840d80c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modeling_distillemb import BertModel, BertForSequenceClassification, BertForEmbeddingLM\n",
    "from distill_emb import DistillEmbSmall, DistillEmb\n",
    "from config import DistillModelConfig, DistillEmbConfig\n",
    "import torch\n",
    "from transformers import AutoTokenizer, RwkvConfig, RwkvModel, AutoModel\n",
    "from tokenizer import CharTokenizer\n",
    "from knn_classifier import KNNTextClassifier\n",
    "from data_loader import load_sentiment, load_ner_dataset, load_pos_dataset\n",
    "from data_loader import load_news_dataset\n",
    "import pandas as pd\n",
    "from retrieval import build_json_pairs, top1_accuracy\n",
    "import os\n",
    "from transformers import GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dccc0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, classes = load_sentiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afb114a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9dc4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, classes = load_pos_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e6dda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf287ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_input_chars=12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54c8fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://huggingface.co/leobitz/distil-emb-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c07d547",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CharTokenizer.from_pretrained(pretrained_directory=\"distil-emb-base\")\n",
    "distill_config = DistillEmbConfig.from_pretrained(pretrained_model_name_or_path=\"distil-emb-base\")\n",
    "distill_model = DistillEmb.from_pretrained(pretrained_model_name_or_path=\"distil-emb-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddb4bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "distill_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0870d762",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = DistillModelConfig(\n",
    "    vocab_size=30522,\n",
    "    hidden_size=768,\n",
    "    num_hidden_layers=3,\n",
    "    num_attention_heads=8,\n",
    "    intermediate_size=3072,\n",
    "    max_position_embeddings=512,\n",
    "    type_vocab_size=2,\n",
    "    pad_token_id=0,\n",
    "    position_embedding_type=\"absolute\",\n",
    "    use_cache=True,\n",
    "    classifier_dropout=None,\n",
    "    embedding_type=\"distill\",  # 'distilemb', 'fasttext'\n",
    "    encoder_type='lstm', #'lstm'\n",
    "    num_input_chars=num_input_chars,  # number of characters in each token\n",
    "    char_vocab_size=tokenizer.char_vocab_size,\n",
    "    distill_config=distill_config,\n",
    "    distill_pretrained_model_name=\"distil-emb-base\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0daae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"downstream-data/masakhanews.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea92ac1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d6f22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = len(df['label'].unique())\n",
    "config.num_labels = num_labels\n",
    "model = BertForSequenceClassification(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9f6fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input ids with (B, S, N)\n",
    "char_input = torch.randint(0, config.num_input_chars, (1, 10, config.num_input_chars))\n",
    "# input ids with (B, S, N)\n",
    "print(\"char_input shape:\", char_input.shape)\n",
    "inputs = {\n",
    "    \"input_ids\": char_input,\n",
    "    \"attention_mask\":torch.tensor([[1] * char_input.size(1)]),  # attention mask for each token\n",
    "    \"token_type_ids\": torch.tensor([[0] * char_input.size(1)]),  # token type ids for each token\n",
    "}\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86b2456",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(\"Hello world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2955b9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fde95b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"distil-emb-seqcls-lstm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a35ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"distil-emb-seqcls-lstm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58e21ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "df['text'] = df['headline']\n",
    "# Assuming df is your dataframe\n",
    "# Split the data based on the 'split' column\n",
    "train_df = df[df['split'] == 'train'][['text', 'label']]\n",
    "test_df = df[df['split'] == 'test'][['text', 'label']]\n",
    "\n",
    "# Create HuggingFace datasets\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad07d908",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4416f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "\n",
    "def preprocess_function(examples: Dict[str, Any]):\n",
    "    batch = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=False,\n",
    "        max_length=512,\n",
    "        return_attention_mask=True,\n",
    "    )\n",
    "\n",
    "    batch[\"labels\"] = examples[\"label\"]\n",
    "    return batch\n",
    "\n",
    "\n",
    "\n",
    "tokenized_train = train_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names,\n",
    ")\n",
    "\n",
    "tokenized_test = test_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=test_dataset.column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc988f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset[0]['text'].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b8335e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "class CustomDataCollator:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, features):\n",
    "        batch = self.tokenizer.pad(\n",
    "            features,\n",
    "            padding=\"longest\",\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\",\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        return batch\n",
    "\n",
    "data_collator = CustomDataCollator(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabfd52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    report_to=[]\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
