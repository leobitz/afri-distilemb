{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "840d80c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert import BertModel\n",
    "from distill_emb import DistillEmbSmall\n",
    "from config import BertConfig\n",
    "import torch\n",
    "from transformers import AutoTokenizer, RwkvConfig, RwkvModel, AutoModel\n",
    "from tokenizer import CharTokenizer\n",
    "from knn_classifier import KNNTextClassifier\n",
    "from data_loader import load_sentiment\n",
    "from data_loader import load_news_dataset\n",
    "import pandas as pd\n",
    "from retrieval import build_json_pairs, top1_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf287ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_input_chars=12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c07d547",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CharTokenizer(charset_file_path='tokenizer/charset.json',\n",
    "                          max_word_length=num_input_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0870d762",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig(\n",
    "    vocab_size=30522,\n",
    "    hidden_size=768,\n",
    "    num_hidden_layers=9,\n",
    "    num_attention_heads=8,\n",
    "    intermediate_size=3072,\n",
    "    max_position_embeddings=512,\n",
    "    type_vocab_size=2,\n",
    "    pad_token_id=0,\n",
    "    position_embedding_type=\"absolute\",\n",
    "    use_cache=True,\n",
    "    classifier_dropout=None,\n",
    "    embedding_type=\"distill\",  # 'distilemb', 'fasttext'\n",
    "    encoder_type='lstm',\n",
    "    num_input_chars=num_input_chars,  # number of characters in each token\n",
    "    char_vocab_size=tokenizer.char_vocab_size\n",
    ")\n",
    "model = BertModel(config)\n",
    "# input ids with (B, S, N)\n",
    "char_input = torch.randint(0, config.num_input_chars, (1, 10, config.num_input_chars))\n",
    "# input ids with (B, S, N)\n",
    "\n",
    "inputs = {\n",
    "    \"input_ids\": char_input,\n",
    "    \"attention_mask\":torch.tensor([[1] * char_input.size(1)]),  # attention mask for each token\n",
    "    \"token_type_ids\": torch.tensor([[0] * char_input.size(1)]),  # token type ids for each token\n",
    "}\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d0e8c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "distill_emb = DistillEmbSmall(config)\n",
    "# distilemb/logs/distill_emb_v0/distill_emb_v0-epoch=18-val_loss=0.00.ckpt\n",
    "# state_dict = torch.load('logs/distill_emb_v0/distill_emb_v0-epoch=18-val_loss=0.00.ckpt')['state_dict']\n",
    "# # remove 'model.' prefix from state_dict keys\n",
    "# state_dict = {k.replace('model.', ''): v for k, v in state_dict.items()}\n",
    "# distill_emb.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f387f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistillEmbSmall(\n",
       "  (embedding): Embedding(1518, 64)\n",
       "  (conv1): Conv1d(12, 128, kernel_size=(5,), stride=(1,))\n",
       "  (conv2): Conv1d(128, 256, kernel_size=(5,), stride=(1,))\n",
       "  (conv3): Conv1d(256, 384, kernel_size=(5,), stride=(1,))\n",
       "  (conv4): Conv1d(384, 512, kernel_size=(4,), stride=(1,))\n",
       "  (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (activation): ReLU()\n",
       "  (tanh): Tanh()\n",
       "  (norm0): LayerNorm((12, 64), eps=1e-05, elementwise_affine=True)\n",
       "  (norm1): LayerNorm((128, 30), eps=1e-05, elementwise_affine=True)\n",
       "  (norm2): LayerNorm((256, 13), eps=1e-05, elementwise_affine=True)\n",
       "  (norm3): LayerNorm((384, 4), eps=1e-05, elementwise_affine=True)\n",
       "  (norm4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (output_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distill_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72f0f39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = tokenizer('hello world', add_special_tokens=False, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af523e55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distill_emb(out['input_ids'][0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f0aeb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "distill_emb = distill_emb.to('cuda').eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51528542",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = KNNTextClassifier(tokenizer, model=distill_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05e484af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 105862 rows from sentiment.parquet columns Index(['text', 'label', 'lang', 'split'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df, classes = load_sentiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5227365",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df.sample(1000, random_state=42)\n",
    "test_df = df.drop(train_df.index).sample(100, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7589f008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3740010340536656,\n",
       " 0.3700000000000001,\n",
       " {'ma': {'f1': 0.384786641929499, 'acc': 0.35714285714285715},\n",
       "  'sw': {'f1': 0.21052631578947367, 'acc': 0.2},\n",
       "  'dz': {'f1': 0.68994708994709, 'acc': 0.6666666666666666},\n",
       "  'kr': {'f1': 0.3181818181818182, 'acc': 0.2727272727272727},\n",
       "  'pt': {'f1': 0.4653061224489795, 'acc': 0.42857142857142855},\n",
       "  'twi': {'f1': 0.1, 'acc': 0.25},\n",
       "  'yo': {'f1': 0.3976331360946746, 'acc': 0.38461538461538464},\n",
       "  'pcm': {'f1': 0.7333333333333334, 'acc': 0.75},\n",
       "  'ha': {'f1': 0.4, 'acc': 0.4},\n",
       "  'am': {'f1': 0.6, 'acc': 0.6},\n",
       "  'ts': {'f1': 0.0, 'acc': 0.0},\n",
       "  'ig': {'f1': 0.3333333333333333, 'acc': 0.5},\n",
       "  'tg': {'f1': 0.0, 'acc': 0.0}})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.classifiy(train_df=train_df, test_df=test_df, k=5, batch_size=32, model=None, tokenizer=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07533b14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f36bec3d214487caceb4a1f143d082a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/241 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leo/miniconda3/envs/py11/lib/python3.11/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d268486cb9f246248c9ee93fb054139b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/755 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "002f235abd264964b149e9c92e128e83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/6.01M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62df77b3f3ac4e0181f02040da1d5ea4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cb0c9e8db9d42c7bd1dd41cc2e53ec1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.06G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at bonadossou/afrolm_active_learning and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.4402293233082707,\n",
       " 0.45000000000000007,\n",
       " {'ma': {'f1': 0.293040293040293, 'acc': 0.2857142857142857},\n",
       "  'sw': {'f1': 0.5898279352226721, 'acc': 0.6},\n",
       "  'dz': {'f1': 0.23703703703703705, 'acc': 0.3333333333333333},\n",
       "  'kr': {'f1': 0.47792207792207786, 'acc': 0.45454545454545453},\n",
       "  'pt': {'f1': 0.42857142857142855, 'acc': 0.42857142857142855},\n",
       "  'twi': {'f1': 0.25, 'acc': 0.25},\n",
       "  'yo': {'f1': 0.6205128205128205, 'acc': 0.6153846153846154},\n",
       "  'pcm': {'f1': 0.3333333333333333, 'acc': 0.25},\n",
       "  'ha': {'f1': 0.4, 'acc': 0.4},\n",
       "  'am': {'f1': 0.6, 'acc': 0.6},\n",
       "  'ts': {'f1': 0.3333333333333333, 'acc': 0.3333333333333333},\n",
       "  'ig': {'f1': 0.3333333333333333, 'acc': 0.5},\n",
       "  'tg': {'f1': 0.0, 'acc': 0.0}})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"bonadossou/afrolm_active_learning\"\n",
    "tok = AutoTokenizer.from_pretrained(model_name)\n",
    "xmodel = AutoModel.from_pretrained(model_name)\n",
    "class Wrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        embs = self.model(**kwargs).last_hidden_state\n",
    "        return embs\n",
    "\n",
    "wrapper_model = Wrapper(xmodel).to('cuda').eval()\n",
    "classifier = KNNTextClassifier(tokenizer=tok, model=wrapper_model)\n",
    "classifier.classifiy(train_df=train_df, test_df=test_df, k=5, batch_size=32, model=wrapper_model, tokenizer=tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f2459c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 30809 rows from masakhanews.parquet columns Index(['label', 'headline', 'text', 'headline_text', 'url', 'lang', 'split'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "data, classes = load_news_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34322e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = data.sample(1000, random_state=42)\n",
    "test_df = data.drop(train_df.index).sample(100, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2893e46",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (512) must match the existing size (258) at non-singleton dimension 1.  Target sizes: [32, 512].  Tensor sizes: [1, 258]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mclassifier\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclassifiy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_df\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtok\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/distil-research/distilemb/knn_classifier.py:84\u001b[39m, in \u001b[36mKNNTextClassifier.classifiy\u001b[39m\u001b[34m(self, model, tokenizer, train_df, test_df, batch_size, k, metric, n_jobs, device)\u001b[39m\n\u001b[32m     81\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.vstack(embeds)\n\u001b[32m     83\u001b[39m \u001b[38;5;66;03m# ---------- 1. Embed & fit ----------\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m X_train = \u001b[43m_batch_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m y_train = train_df[\u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m].values  \u001b[38;5;66;03m# string or int labels both fine\u001b[39;00m\n\u001b[32m     87\u001b[39m knn = KNeighborsClassifier(\n\u001b[32m     88\u001b[39m     n_neighbors=k, metric=metric, n_jobs=n_jobs\n\u001b[32m     89\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/distil-research/distilemb/knn_classifier.py:78\u001b[39m, in \u001b[36mKNNTextClassifier.classifiy.<locals>._batch_embed\u001b[39m\u001b[34m(text_series)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m start \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(text_series), batch_size):\n\u001b[32m     77\u001b[39m     batch_texts = text_series.iloc[start:start + batch_size].tolist()\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     batch_emb = \u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_texts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m     embeds.append(batch_emb)\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m np.vstack(embeds)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/distil-research/distilemb/knn_classifier.py:63\u001b[39m, in \u001b[36mKNNTextClassifier.classifiy.<locals>.embed\u001b[39m\u001b[34m(texts)\u001b[39m\n\u001b[32m     60\u001b[39m tokenized = tokenizer(texts, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m, padding=\u001b[38;5;28;01mTrue\u001b[39;00m, truncation=\u001b[38;5;28;01mTrue\u001b[39;00m, max_length=\u001b[32m512\u001b[39m)\n\u001b[32m     61\u001b[39m tokenized = {k: v.to(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m tokenized.items()}\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m batch_emb = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtokenized\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# Ensure numpy float32 on CPU for sklearn\u001b[39;00m\n\u001b[32m     66\u001b[39m batch_emb = (\n\u001b[32m     67\u001b[39m     batch_emb.detach().cpu().numpy()  \u001b[38;5;66;03m# if torch.Tensor\u001b[39;00m\n\u001b[32m     68\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(batch_emb, \u001b[33m\"\u001b[39m\u001b[33mdetach\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     69\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m np.asarray(batch_emb)\n\u001b[32m     70\u001b[39m ).astype(np.float32).mean(axis=\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# average over tokens if needed\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py11/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py11/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py11/lib/python3.11/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py:800\u001b[39m, in \u001b[36mXLMRobertaModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    798\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.embeddings, \u001b[33m\"\u001b[39m\u001b[33mtoken_type_ids\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    799\u001b[39m     buffered_token_type_ids = \u001b[38;5;28mself\u001b[39m.embeddings.token_type_ids[:, :seq_length]\n\u001b[32m--> \u001b[39m\u001b[32m800\u001b[39m     buffered_token_type_ids_expanded = \u001b[43mbuffered_token_type_ids\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    801\u001b[39m     token_type_ids = buffered_token_type_ids_expanded\n\u001b[32m    802\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mRuntimeError\u001b[39m: The expanded size of the tensor (512) must match the existing size (258) at non-singleton dimension 1.  Target sizes: [32, 512].  Tensor sizes: [1, 258]"
     ]
    }
   ],
   "source": [
    "classifier.classifiy(train_df=train_df, test_df=test_df, k=5, batch_size=32, model=xmodel, tokenizer=tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "46e4b474",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_92455/1079180312.py:2: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train_df = data[data['split'] == 'train'].groupby('lang').apply(lambda x: x.sample(200, random_state=42)).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "# select 10 per language\n",
    "train_df = data[data['split'] == 'train'].groupby('lang').apply(lambda x: x.sample(200, random_state=42)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c81b19d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# langs = ['amh', 'hau', 'ibo', 'lug', 'pcm','yor']\n",
    "# train_df = train_df[train_df['lang'].isin(langs)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75066ca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Varimi vePurazi reWadzanayi kuLalapanzi Vonetsana neKambani yekuChina YeAsia Ferry Yakapinda muPurazi Ravo'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['headline'].sample(1).values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a1b56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = build_json_pairs(train_df, model_name=\"Davlan/afro-xlmr-large\",\n",
    "#                  n_samples=200, m_candidates=100, k_top=9, text_col=\"text\", headline_col=\"headline\")\n",
    "# # save to json file\n",
    "# import json\n",
    "# with open('news_result.json', 'w', encoding='utf-8') as f:\n",
    "#     json.dump(result, f, indent=4, ensure_ascii=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46753293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = build_json_pairs(train_df, model_name=\"Davlan/afro-xlmr-large\",\n",
    "#                  n_samples=200, m_candidates=100, k_top=9, text_col=\"headline\", headline_col=\"text\")\n",
    "# # save to json file\n",
    "# import json\n",
    "# with open('headline_result.json', 'w', encoding='utf-8') as f:\n",
    "#     json.dump(result, f, indent=4, ensure_ascii=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "076d75e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f86ab525e694b9489b8b9ef58de2dbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (512) must match the existing size (258) at non-singleton dimension 1.  Target sizes: [1, 512].  Tensor sizes: [1, 258]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m df = pd.read_json(\u001b[33m'\u001b[39m\u001b[33mnews_result.json\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      2\u001b[39m d = df.to_dict(orient=\u001b[33m'\u001b[39m\u001b[33mrecords\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mtop1_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtok\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# top1_accuracy(d, batch_size=32, model=distill_emb, tokenizer=tokenizer)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/distil-research/distilemb/retrieval.py:215\u001b[39m, in \u001b[36mtop1_accuracy\u001b[39m\u001b[34m(qa_pairs, batch_size, model, tokenizer)\u001b[39m\n\u001b[32m    213\u001b[39m lang = obj[\u001b[33m\"\u001b[39m\u001b[33mlang\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    214\u001b[39m \u001b[38;5;66;03m# get embeddings\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m query_emb = \u001b[43mencoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mquery_text\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]          \u001b[38;5;66;03m# (d,)\u001b[39;00m\n\u001b[32m    216\u001b[39m cand_embs = encoder.encode(candidates, batch_size)   \u001b[38;5;66;03m# (K+1, d)\u001b[39;00m\n\u001b[32m    218\u001b[39m \u001b[38;5;66;03m# cosine similarities\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py11/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/distil-research/distilemb/retrieval.py:50\u001b[39m, in \u001b[36mHFEncoder.encode\u001b[39m\u001b[34m(self, sentences, batch_size)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# print({k:v.shape for k, v in enc.items()})\u001b[39;00m\n\u001b[32m     49\u001b[39m enc = {k: v.to(\u001b[38;5;28mself\u001b[39m.device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m enc.items()}\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43menc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(result) \u001b[38;5;129;01mis\u001b[39;00m torch.Tensor:\n\u001b[32m     52\u001b[39m     emb = result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py11/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py11/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/py11/lib/python3.11/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py:800\u001b[39m, in \u001b[36mXLMRobertaModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    798\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.embeddings, \u001b[33m\"\u001b[39m\u001b[33mtoken_type_ids\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    799\u001b[39m     buffered_token_type_ids = \u001b[38;5;28mself\u001b[39m.embeddings.token_type_ids[:, :seq_length]\n\u001b[32m--> \u001b[39m\u001b[32m800\u001b[39m     buffered_token_type_ids_expanded = \u001b[43mbuffered_token_type_ids\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    801\u001b[39m     token_type_ids = buffered_token_type_ids_expanded\n\u001b[32m    802\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mRuntimeError\u001b[39m: The expanded size of the tensor (512) must match the existing size (258) at non-singleton dimension 1.  Target sizes: [1, 512].  Tensor sizes: [1, 258]"
     ]
    }
   ],
   "source": [
    "df = pd.read_json('news_result.json')\n",
    "d = df.to_dict(orient='records')\n",
    "top1_accuracy(d, batch_size=32, model=xmodel, tokenizer=tok)\n",
    "# top1_accuracy(d, batch_size=32, model=distill_emb, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db7f662",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fasttext_model import FastTextModel\n",
    "fasttext_model = FastTextModel(file_path='embeddings/afriberta/afriberta.vec')\n",
    "# fasttext_model.embedding.weight.requires_grad = False  # freeze the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a441e21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
