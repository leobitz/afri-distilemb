{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bb64b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert import BertModel\n",
    "from distill_emb import DistillEmbSmall\n",
    "from config import DistilEmbConfig\n",
    "import torch\n",
    "from transformers import AutoTokenizer, RwkvConfig, RwkvModel, AutoModel\n",
    "from tokenizer import CharTokenizer\n",
    "from knn_classifier import KNNTextClassifier\n",
    "from data_loader import load_sentiment\n",
    "from data_loader import load_news_dataset\n",
    "import pandas as pd\n",
    "from retrieval import build_json_pairs, top1_accuracy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "981afb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_input_chars=12\n",
    "tokenizer = CharTokenizer(charset_file_path='tokenizer/charset.json',\n",
    "                          max_word_length=num_input_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d784e0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = DistilEmbConfig(\n",
    "    vocab_size=30522,\n",
    "    hidden_size=768,\n",
    "    num_hidden_layers=9,\n",
    "    num_attention_heads=8,\n",
    "    intermediate_size=3072,\n",
    "    max_position_embeddings=512,\n",
    "    type_vocab_size=2,\n",
    "    pad_token_id=0,\n",
    "    position_embedding_type=\"absolute\",\n",
    "    use_cache=True,\n",
    "    classifier_dropout=None,\n",
    "    embedding_type=\"distill\",  # 'distilemb', 'fasttext'\n",
    "    encoder_type='lstm',\n",
    "    num_input_chars=num_input_chars,  # number of characters in each token\n",
    "    char_vocab_size=tokenizer.char_vocab_size\n",
    ")\n",
    "distill_emb = DistillEmbSmall(config)\n",
    "path = \"logs/distill_emb_v0/distill_emb_v0-epoch=510-epoch_val_loss=0.27.ckpt\"\n",
    "if os.path.exists(path):\n",
    "    state_dict = torch.load(path, map_location='cpu')['state_dict']\n",
    "    # remove 'model.' prefix from state_dict keys\n",
    "    state_dict = {k.replace('model.', ''): v for k, v in state_dict.items()}\n",
    "    distill_emb.load_state_dict(state_dict)\n",
    "else:\n",
    "    print(f\"Model checkpoint {path} not found. Please check the path.\")\n",
    "\n",
    "distill_emb = distill_emb.to('cuda').eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ded031a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, tokenizer):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        classifier = KNNTextClassifier(tokenizer, model=model)\n",
    "\n",
    "        df, classes = load_sentiment()\n",
    "        # Sample equal amount for each language in the 'lang' column\n",
    "        min_count = min(df['lang'].value_counts().min(), 250)\n",
    "        sent_df = df.groupby('lang').apply(lambda x: x.sample(min_count, random_state=42)).reset_index(drop=True)\n",
    "        sent_train_df = sent_df.sample(frac=0.8, random_state=42)\n",
    "        sent_test_df = sent_df.drop(sent_train_df.index)\n",
    "        print(f\"train shape: {sent_train_df.shape}, test shape: {sent_test_df.shape}\")\n",
    "        sent_f1, sent_acc, sent_per_lang, sent_test_df = classifier.classifiy(train_df=sent_train_df, test_df=sent_test_df, k=5, batch_size=32, model=None, tokenizer=None)\n",
    "\n",
    "        df, classes = load_news_dataset()\n",
    "        min_count = min(df['lang'].value_counts().min(), 250)\n",
    "        news_df = df.groupby('lang').apply(lambda x: x.sample(min_count, random_state=42)).reset_index(drop=True)\n",
    "        news_train_df = news_df.sample(frac=0.8, random_state=42)\n",
    "        news_test_df = news_df.drop(news_train_df.index)\n",
    "        print(f\"train shape: {news_train_df.shape}, test shape: {news_test_df.shape}\")\n",
    "        news_f1, news_acc, news_per_lang, news_test_df = classifier.classifiy(train_df=news_train_df, test_df=news_test_df, k=5, batch_size=32, model=None, tokenizer=None)\n",
    "\n",
    "        df = pd.read_json('downstream-data/news_result.json')\n",
    "        d = df.to_dict(orient='records')\n",
    "        ret_acc, _, ret_per_lang = top1_accuracy(d, batch_size=32, model=model, tokenizer=tokenizer)\n",
    "    \n",
    "    return sent_acc, news_acc, ret_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc257006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 105862 rows from sentiment.parquet columns Index(['text', 'label', 'lang', 'split'], dtype='object')\n",
      "train shape: (2800, 4), test shape: (700, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_506039/3986928702.py:9: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sent_df = df.groupby('lang').apply(lambda x: x.sample(min_count, random_state=42)).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 30809 rows from masakhanews.parquet columns Index(['label', 'headline', 'text', 'headline_text', 'url', 'lang', 'split'], dtype='object')\n",
      "train shape: (3200, 7), test shape: (800, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_506039/3986928702.py:17: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  news_df = df.groupby('lang').apply(lambda x: x.sample(min_count, random_state=42)).reset_index(drop=True)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "889d0b137fe84f8a90e198200e951f17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(0.44999999999999996, 0.5387499999999998, 0.174375)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_model(distill_emb, tokenizer)\n",
    "# (0.45428571428571435, 0.51125, 0.1453125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d42461f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        embs = self.model(**kwargs).last_hidden_state\n",
    "        return embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "303a7dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leo/miniconda3/envs/py11/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at castorini/afriberta_small and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_798080/3986928702.py:9: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sent_df = df.groupby('lang').apply(lambda x: x.sample(min_count, random_state=42)).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 105862 rows from sentiment.parquet columns Index(['text', 'label', 'lang', 'split'], dtype='object')\n",
      "train shape: (2800, 4), test shape: (700, 4)\n",
      "Loaded 30809 rows from masakhanews.parquet columns Index(['label', 'headline', 'text', 'headline_text', 'url', 'lang', 'split'], dtype='object')\n",
      "train shape: (3200, 7), test shape: (800, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_798080/3986928702.py:17: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  news_df = df.groupby('lang').apply(lambda x: x.sample(min_count, random_state=42)).reset_index(drop=True)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff55f766aadf4de1b866219565dafb42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(0.47428571428571425, 0.6525, 0.244375)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"castorini/afriberta_small\"\n",
    "tok = AutoTokenizer.from_pretrained(model_name)\n",
    "xmodel = AutoModel.from_pretrained(model_name)\n",
    "xmodel = Wrapper(xmodel)\n",
    "eval_model(xmodel, tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21764c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leo/miniconda3/envs/py11/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at castorini/afriberta_large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 105862 rows from sentiment.parquet columns Index(['text', 'label', 'lang', 'split'], dtype='object')\n",
      "train shape: (2800, 4), test shape: (700, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_798080/3986928702.py:9: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sent_df = df.groupby('lang').apply(lambda x: x.sample(min_count, random_state=42)).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 30809 rows from masakhanews.parquet columns Index(['label', 'headline', 'text', 'headline_text', 'url', 'lang', 'split'], dtype='object')\n",
      "train shape: (3200, 7), test shape: (800, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_798080/3986928702.py:17: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  news_df = df.groupby('lang').apply(lambda x: x.sample(min_count, random_state=42)).reset_index(drop=True)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d2d96e1068542809eff461d18ea4d08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(0.4671428571428571, 0.6612500000000001, 0.285)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"castorini/afriberta_large\"\n",
    "tok = AutoTokenizer.from_pretrained(model_name)\n",
    "xmodel = AutoModel.from_pretrained(model_name)\n",
    "xmodel = Wrapper(xmodel)\n",
    "eval_model(xmodel, tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa804c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
